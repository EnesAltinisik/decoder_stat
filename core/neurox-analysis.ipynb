{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.insert(0,'../NeuroX/')\n",
    "from neurox.data.extraction import transformers_extractor\n",
    "import neurox.data.loader as data_loader\n",
    "import neurox.interpretation.ablation as ablation\n",
    "import neurox.interpretation.linear_probe as linear_probe\n",
    "import neurox.interpretation.utils as utils\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reshape(dataset, method,avg):\n",
    "    activations = {}\n",
    "    for split in ['train','test']:\n",
    "        file = f'data/llama-2/{dataset}/{method}/activations-{split}-{avg}.json'\n",
    "        if not os.path.exists(file):\n",
    "            continue\n",
    "        activation,_ = data_loader.load_activations(file, 4096)\n",
    "        activations[split] = np.array(activation)\n",
    "    return activations\n",
    "    \n",
    "def load_labels(dataset, method='echo',avg='mean'):\n",
    "    labels = {}\n",
    "    for split in ['train','test']:\n",
    "        file = f'data/llama-2/{dataset}/{method}/labels-{split}.pth'\n",
    "        if not os.path.exists(file):\n",
    "            continue\n",
    "        labels[split] = pc.load(open(file,'rb'))\n",
    "    return labels\n",
    "\n",
    "def get_activations(method,avg):\n",
    "    activations_toxic = load_reshape('toxic', method,avg)\n",
    "    activations_xstest = load_reshape('xstest', method,avg)\n",
    "    activations_adv = load_reshape('adv', method,avg)\n",
    "    activations_mt = load_reshape('mt', method,avg)\n",
    "    return activations_toxic,activations_xstest, activations_adv, activations_mt\n",
    "                                      \n",
    "def get_labels():\n",
    "    toxic_labels = load_labels('toxic')\n",
    "    xstest_labels = load_labels('xstest')\n",
    "    adv_labels = load_labels('adv')\n",
    "    mt_labels = load_labels('mt')\n",
    "    return toxic_labels, xstest_labels, adv_labels, mt_labels\n",
    "               \n",
    "def get_activation_dict(method,avg):\n",
    "    activations_toxic,activations_xstest, activations_adv, activations_mt = get_activations(method,avg)\n",
    "    activations = {'train':activations_toxic['train'], \n",
    "                   'test_toxic':activations_toxic['test'],\n",
    "                   'test_xstest':activations_xstest['test'],\n",
    "                   'test_adv':activations_adv['test'], 'test_mt':activations_mt['test']}\n",
    "    return activations\n",
    "\n",
    "def get_labels_dict(method,avg):\n",
    "    toxic_labels, xstest_labels, adv_labels,mt_labels = get_labels()\n",
    "    labels = {'train':toxic_labels['train'], \n",
    "               'test_toxic':toxic_labels['test'],\n",
    "               'test_xstest':xstest_labels['test'],\n",
    "               'test_adv':adv_labels['test'], 'test_mt':mt_labels['test']}\n",
    "    return labels\n",
    "                                      \n",
    "def get_activations_labels(method,avg):\n",
    "    activations = get_activation_dict(method,avg)\n",
    "    labels = get_labels_dict(method,avg)\n",
    "    return activations, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_data(activation, label):\n",
    "    target = [[c] for c in label]#[740:]\n",
    "    source = [np.array([c]) for c in label]\n",
    "    tokens = {\"source\": source, \"target\": target}\n",
    "    X_train, y_train, mapping = utils.create_tensors(tokens, activation, 1)\n",
    "    label2idx, idx2label, src2idx, idx2src = mapping\n",
    "    return {'X':X_train, 'y':y_train, 'label2idx':label2idx , 'idx2label':idx2label}\n",
    "def get_train_test_data(activations, labels):\n",
    "    train_test_data = {}\n",
    "    for split in labels:\n",
    "        train_test_data[split] = get_split_data(activations[split], labels[split])\n",
    "    return train_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_curve, auc\n",
    "def calculate_f1(true_labels, predicted_labels):\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    "\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    return [precision, recall, f1]\n",
    "def get_metrics(true_labels,outputs_all):\n",
    "    # Calculate AUPRC\n",
    "    precision, recall, thresholds = precision_recall_curve(true_labels,outputs_all[:,1])\n",
    "    auprc = auc(recall, precision)\n",
    "\n",
    "\n",
    "    # Calculate Precision, Recall, F1\n",
    "    # predicted_labels = np.argmax(outputs_all,axis=1)# \n",
    "    best_f1 = 0\n",
    "    best_thr = 0\n",
    "    for thr in thresholds:\n",
    "        predicted_labels = [1 if feature >=thr else 0 for feature in outputs_all[:,1]]\n",
    "        f1 = f1_score(true_labels, predicted_labels)\n",
    "        if f1>best_f1:\n",
    "            best_f1=f1\n",
    "            best_thr = thr\n",
    "\n",
    "    predicted_labels = [1 if feature >=best_thr else 0 for feature in outputs_all[:,1]]\n",
    "    res={}\n",
    "    res['best']=calculate_f1(true_labels, predicted_labels)+[auprc]\n",
    "    predicted_labels = np.argmax(outputs_all,axis=1)\n",
    "    res['normal']=calculate_f1(true_labels, predicted_labels)+[auprc]\n",
    "    return res\n",
    "    \n",
    "\n",
    "def get_layer_data(layer, train_test_data):\n",
    "    layer_data = {}\n",
    "    for split in train_test_data:\n",
    "        X = train_test_data[split]['X']\n",
    "        if layer == -1:\n",
    "            layer_X = X\n",
    "        else:\n",
    "            layer_X = ablation.filter_activations_by_layers(X, [layer], 33)\n",
    "        layer_data[split]=layer_X\n",
    "    return layer_data\n",
    "\n",
    "def run_test(y_test, outputs_all):\n",
    "    return get_metrics(y_test,outputs_all)\n",
    "    auprc = average_precision_score(y_test, outputs_all[:,1])\n",
    "    p,r,f1,_ = precision_recall_fscore_support(y_test, np.argmax(outputs_all,axis=1), average='macro')\n",
    "    return p,r,f1,auprc\n",
    "\n",
    "def get_test_scores(probe,train_test_data,layer_data,split):        \n",
    "    _,_, outputs_all=linear_probe.evaluate_probe(probe, layer_data[split], train_test_data[split]['y'], idx_to_class=train_test_data['train']['idx2label'],return_predictions=True)\n",
    "    return run_test(train_test_data[split]['y'], outputs_all), outputs_all\n",
    "    \n",
    "# def get_score_filtered(probe, train_test_data,layer_data, split, n=100):\n",
    "#     X_test = ablation.filter_activations_keep_neurons(layer_data[split], ordering[:n])\n",
    "#     probe_layer_0 = linear_probe.train_logistic_regression_probe(X_train, y_train, lambda_l1=0.001, lambda_l2=0.001)\n",
    "#     class_score, prediction, outputs_all=linear_probe.evaluate_probe(probe_layer_0, X_test, y_test, idx_to_class=idx2label,return_predictions=True)\n",
    "#     return get_scores(y_test, outputs_all)\n",
    "\n",
    "\n",
    "def get_test_scores_filtered(probe,ordering,train_test_data,layer_data,split,n):\n",
    "    X_test = ablation.filter_activations_keep_neurons(layer_data[split], ordering[:n])\n",
    "    _,_, outputs_all=linear_probe.evaluate_probe(probe, X_test, train_test_data[split]['y'], idx_to_class=train_test_data['train']['idx2label'],return_predictions=True)\n",
    "    return run_test(train_test_data[split]['y'], outputs_all), outputs_all\n",
    "    \n",
    "def run_filtered(probe_layer,train_test_data,layer_data,n):\n",
    "    ordering, cutoffs = linear_probe.get_neuron_ordering(probe_layer, train_test_data['train']['idx2label'])\n",
    "    X_train = ablation.filter_activations_keep_neurons(layer_data['train'], ordering[:n])\n",
    "    probe_layer = linear_probe.train_logistic_regression_probe(X_train, train_test_data['train']['y'], lambda_l1=0.001, lambda_l2=0.001)\n",
    "    results = {}\n",
    "    for split in ['test_toxic','test_xstest','test_adv','test_mt']:\n",
    "        results[split] = get_test_scores_filtered(probe,ordering,train_test_data,layer_data,split,n)\n",
    "    return results[split]   \n",
    "        \n",
    "    \n",
    "def run_layer(train_test_data,layer, n=100):\n",
    "    layer_data = get_layer_data(layer, train_test_data)\n",
    "    probe_layer = linear_probe.train_logistic_regression_probe(layer_data['train'], train_test_data['train']['y'], lambda_l1=0.001, lambda_l2=0.001)\n",
    "    results = {}\n",
    "    for split in ['test_toxic','test_xstest','test_adv','test_mt']:\n",
    "        results[split] = get_test_scores(probe_layer,train_test_data,layer_data,split)\n",
    "    # results['filtered'] = run_filtered(probe,train_test_data,layer_data,n)\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_auprc(results,title):\n",
    "    res = {'test_toxic':{},'test_xstest':{},'test_adv':{},'test_mt':{}}\n",
    "    for layer in results:\n",
    "        for split in results[layer]:\n",
    "            if split=='test_adv':\n",
    "                res[split][layer]=np.mean(np.argmax(results[layer][split][1],axis=1)==1) \n",
    "            elif split=='test_mt':\n",
    "                res[split][layer]=np.mean(np.argmax(results[layer][split][1],axis=1)==0) \n",
    "            else:\n",
    "                res[split][layer]=results[layer][split][0]['best'][-1]                \n",
    "    for split in res:\n",
    "        plt.plot(list(res[split].keys())[1:],list(res[split].values())[1:],label=split)\n",
    "        plt.plot([0,33],[list(res[split].values())[0],list(res[split].values())[0]],color='black',linestyle='-.')\n",
    "        if split in base:\n",
    "            plt.plot([0,33],[base[split],base[split]],color='red',linestyle='-.')\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.savefig(f'outs/{title}.pdf',dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_sampels = 768 ###it is dataset dependens (384*2) ##len(questions_dic['train'])\n",
    "method2last_mean = {'echo':['mean'],  ### for echo only average embedding sugested\n",
    "                    'sure':['last'], ### for sure, we only need to embedding for sure\n",
    "                    'standard':['last','mean','weighted']}\n",
    "base = {'test_toxic':0.816,'test_xstest':0.936}## baselines\n",
    "results_all = {}\n",
    "for method in ['echo','standard','sure']:\n",
    "    for avg in method2last_mean[method]:\n",
    "        activations, labels = get_activations_labels(method,avg)\n",
    "        train_test_data = get_train_test_data(activations, labels)\n",
    "        results = {}\n",
    "        for layer in range(-1,33):  ##-1 for all layers\n",
    "            results[layer] = run_layer(train_test_data,layer, n=100)\n",
    "        draw_auprc(results,f'{method}_{avg}')   \n",
    "        results_all[f'{method}{avg}']=results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llm-attack]",
   "language": "python",
   "name": "conda-env-llm-attack-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
